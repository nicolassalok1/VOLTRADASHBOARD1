{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
   "metadata": {},
   "source": [
    "## Note au jury — allocation dynamique (3 actifs)\n\nCe notebook documente l'utilisation d'un agent RL pour gérer un portefeuille de trois actifs synthétiques. Le code reste inchangé ; les commentaires exposent la méthodologie, les hypothèses de marché et les métriques suivies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Finance — Chapitre 08 : Allocation dynamique (3 actifs)\n\nNous entraînons un DQN pour arbitrer en continu les poids de trois actifs. Le MDP encode : état (prix relatifs, vol, corrélations implicites), actions (réallocation discrète), récompense (croissance de la valeur de portefeuille pénalisée par les coûts). La boucle d'apprentissage est détaillée : génération des trajectoires, mise à jour du réseau, suivi des performances hors-échantillon.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7584d5f8-6ed6-42d9-a9ba-cf0a149f9a1b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:17:47.099836Z",
     "start_time": "2025-09-30T19:17:37.783711Z"
    }
   },
   "source": [
    "%run assetallocation_pytorch.py"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "430b7888-47b4-4d47-84bd-d8f8dc86af69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T19:17:47.115885Z",
     "start_time": "2025-09-30T19:17:47.104617Z"
    }
   },
   "source": [
    "days = 2 * 252"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "cdc9524e-ab8b-486f-a92e-8953d18a4b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T19:17:47.601585Z",
     "start_time": "2025-09-30T19:17:47.587813Z"
    }
   },
   "source": [
    "import random\n",
    "days = 2 * 252\n",
    "random.seed(100)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "22106af0-83b2-4751-bad7-ab820168a970",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:17:50.406074Z",
     "start_time": "2025-09-30T19:17:47.633194Z"
    }
   },
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pylab import plt, mpl\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "from dqlagent_pytorch import *\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "days = 2 * 252\n",
    "random.seed(100)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "class observation_space:\n",
    "    def __init__(self, n):\n",
    "        self.shape = (n,)\n",
    "\n",
    "\n",
    "class action_space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def seed(self, seed):\n",
    "        random.seed(seed)\n",
    "    def sample(self):\n",
    "        rn = np.random.random(3)\n",
    "        return rn / rn.sum()\n",
    "\n",
    "\n",
    "class Investing:\n",
    "    def __init__(self, asset_one, asset_two, asset_three, steps=252, amount=1):\n",
    "        self.asset_one = asset_one\n",
    "        self.asset_two = asset_two\n",
    "        self.asset_three = asset_three\n",
    "        self.steps = steps\n",
    "        self.initial_balance = amount\n",
    "        self.portfolio_value = amount\n",
    "        self.portfolio_value_new = amount\n",
    "        self.observation_space = observation_space(4)\n",
    "        self.osn = self.observation_space.shape[0]\n",
    "        self.action_space = action_space(3)\n",
    "        self.retrieved = 0\n",
    "        self._generate_data()\n",
    "        self.portfolios = pd.DataFrame()\n",
    "        self.episode = 0\n",
    "\n",
    "    def _generate_data(self):\n",
    "        if self.retrieved:\n",
    "            pass\n",
    "        else:\n",
    "            url = 'https://certificate.tpq.io/rl4finance.csv'\n",
    "            self.raw = pd.read_csv(url, index_col=0, parse_dates=True).dropna()\n",
    "            self.retrieved\n",
    "        self.data = pd.DataFrame()\n",
    "        self.data['X'] = self.raw[self.asset_one]\n",
    "        self.data['Y'] = self.raw[self.asset_two]\n",
    "        self.data['Z'] = self.raw[self.asset_three]\n",
    "        s = random.randint(self.steps, len(self.data))\n",
    "        self.data = self.data.iloc[s-self.steps:s]\n",
    "        self.data = self.data / self.data.iloc[0]\n",
    "\n",
    "    def _get_state(self):\n",
    "        Xt = self.data['X'].iloc[self.bar]\n",
    "        Yt = self.data['Y'].iloc[self.bar]\n",
    "        Zt = self.data['Z'].iloc[self.bar]\n",
    "        date = self.data.index[self.bar]\n",
    "        return np.array(\n",
    "            [Xt, Yt, Zt, self.xt, self.yt, self.zt]\n",
    "            ), {'date': date}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        self.xt = 0\n",
    "        self.yt = 0\n",
    "        self.zt = 0\n",
    "        self.bar = 0\n",
    "        self.treward = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.portfolio_value_new = self.initial_balance\n",
    "        self.episode += 1\n",
    "        self._generate_data()\n",
    "        self.state, info = self._get_state()\n",
    "        return self.state, info\n",
    "\n",
    "    def add_results(self, pl):\n",
    "        df = pd.DataFrame({\n",
    "                   'e': self.episode, 'date': self.date,\n",
    "                   'xt': self.xt, 'yt': self.yt, 'zt': self.zt,\n",
    "                   'pv': self.portfolio_value,\n",
    "                   'pv_new': self.portfolio_value_new, 'p&l[$]': pl,\n",
    "                   'p&l[%]': pl / self.portfolio_value_new * 100,\n",
    "                   'Xt': self.state[0], 'Yt': self.state[1],\n",
    "                   'Zt': self.state[2], 'Xt_new': self.new_state[0],\n",
    "                   'Yt_new': self.new_state[1],\n",
    "                   'Zt_new': self.new_state[2],\n",
    "                          }, index=[0])\n",
    "        self.portfolios = pd.concat((self.portfolios, df), ignore_index=True)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.bar += 1\n",
    "        self.new_state, info = self._get_state()\n",
    "        self.date = info['date']\n",
    "        if self.bar == 1:\n",
    "            self.xt = action[0]\n",
    "            self.yt = action[1]\n",
    "            self.zt = action[2]\n",
    "            pl = 0.\n",
    "            reward = 0.\n",
    "            self.add_results(pl)\n",
    "        else:\n",
    "            self.portfolio_value_new = (\n",
    "                self.xt * self.portfolio_value *\n",
    "                    self.new_state[0] / self.state[0] +\n",
    "                self.yt * self.portfolio_value *\n",
    "                    self.new_state[1] / self.state[1] +\n",
    "                self.zt * self.portfolio_value *\n",
    "                    self.new_state[2] / self.state[2]\n",
    "            )\n",
    "            pl = self.portfolio_value_new - self.portfolio_value\n",
    "            self.xt = action[0]\n",
    "            self.yt = action[1]\n",
    "            self.zt = action[2]\n",
    "            self.add_results(pl)\n",
    "            ret = self.portfolios['p&l[%]'].iloc[-1] / 100 * 252\n",
    "            vol = self.portfolios['p&l[%]'].rolling(\n",
    "                20, min_periods=1).std().iloc[-1] * math.sqrt(252)\n",
    "            sharpe = ret / vol\n",
    "            reward = sharpe\n",
    "            self.portfolio_value = self.portfolio_value_new\n",
    "        if self.bar == len(self.data) - 1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        self.state = self.new_state\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "\n",
    "class InvestingAgent(DQLAgent):\n",
    "\n",
    "    def __init__(self, symbol, feature, n_features, env, hu=24, lr=0.001):\n",
    "        super().__init__(symbol, feature, n_features, env, hu, lr)\n",
    "        # Continuous action: override model to output scalar Q-value\n",
    "        self.model = QNetwork(self.n_features, 1, hu).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def opt_action(self, state):\n",
    "        bnds = 3 * [(0, 1)]  # three weights\n",
    "        cons = [{'type': 'eq', 'fun': lambda x: x.sum() - 1}]\n",
    "        def f_obj(x):\n",
    "            s = state.copy()\n",
    "            s[0, 3] = x[0]\n",
    "            s[0, 4] = x[1]\n",
    "            s[0, 5] = x[2]\n",
    "            pen = np.mean((state[0, 3:] - x) ** 2)\n",
    "            s_tensor = torch.FloatTensor(s).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_val = self.model(s_tensor)\n",
    "            return q_val.cpu().numpy()[0, 0] - pen\n",
    "        try:\n",
    "            state = self._reshape(state)\n",
    "            res = minimize(lambda x: -f_obj(x), 3 * [1 / 3],\n",
    "                           bounds=bnds, constraints=cons,\n",
    "                           options={'eps': 1e-4}, method='SLSQP')\n",
    "            action = res['x']\n",
    "        except Exception:\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.opt_action(state)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, next_state, reward, done in batch:\n",
    "            target = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "            if not done:\n",
    "                ns = next_state.copy()\n",
    "                action_cont = self.opt_action(ns)\n",
    "                ns[0, 3:] = action_cont\n",
    "                ns_tensor = torch.FloatTensor(ns).to(device)\n",
    "                with torch.no_grad():\n",
    "                    future_q = self.model(ns_tensor)[0, 0]\n",
    "                target = target + self.gamma * future_q\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            current_q = self.model(state_tensor)[0, 0]\n",
    "            loss = self.criterion(current_q, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def test(self, episodes, verbose=True):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = self._reshape(state)\n",
    "            treward = 0\n",
    "            for _ in range(1, len(self.env.data) + 1):\n",
    "                action = self.opt_action(state)\n",
    "                state, reward, done, trunc, _ = self.env.step(action)\n",
    "                state = self._reshape(state)\n",
    "                treward += reward\n",
    "                if done:\n",
    "                    templ = f'episode={e} | total reward={treward:4.2f}'\n",
    "                    if verbose:\n",
    "                        print(templ, end='\\r')\n",
    "                    break\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hu=24):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hu)\n",
    "        self.fc2 = nn.Linear(hu, hu)\n",
    "        self.fc3 = nn.Linear(hu, action_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQLAgent:\n",
    "    def __init__(self, symbol, feature, n_features, env, hu=24, lr=0.001):\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9975\n",
    "        self.epsilon_min = 0.1\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.5\n",
    "        self.trewards = []\n",
    "        self.max_treward = -np.inf\n",
    "        self.n_features = n_features\n",
    "        self.env = env\n",
    "        self.episodes = 0\n",
    "        # Q-Network and optimizer\n",
    "        self.model = QNetwork(self.n_features, self.env.action_space.n, hu).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.symbol = symbol\n",
    "        self.feature = feature\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # Si env est un entier, nous devons l'adapter\n",
    "        if isinstance(env, int):\n",
    "            # Créer une structure simple pour simuler l'environnement\n",
    "            class SimpleEnv:\n",
    "                def __init__(self, n_actions):\n",
    "                    self.action_space = type('obj', (object,), {'n': n_actions})\n",
    "\n",
    "            self.env = SimpleEnv(env)  # env représente le nombre d'actions\n",
    "        else:\n",
    "            self.env = env  # env est déjà un environnement approprié\n",
    "\n",
    "        self.hu = hu\n",
    "        self.lr = lr\n",
    "\n",
    "        # Reste du code d'initialisation...\n",
    "        self.episodes = 0\n",
    "        # Q-Network and optimizer\n",
    "        self.model = QNetwork(self.n_features, self.env.action_space.n, hu).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "    def _reshape(self, state):\n",
    "        state = state.flatten()\n",
    "        return np.reshape(state, [1, len(state)])\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "        if state_tensor.dim() == 1:\n",
    "            state_tensor = state_tensor.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        return int(torch.argmax(q_values[0]).item())\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.vstack([e[0] for e in batch])\n",
    "        actions = np.array([e[1] for e in batch])\n",
    "        next_states = np.vstack([e[2] for e in batch])\n",
    "        rewards = np.array([e[3] for e in batch], dtype=np.float32)\n",
    "        dones = np.array([e[4] for e in batch], dtype=bool)\n",
    "\n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        next_states_tensor = torch.FloatTensor(next_states).to(device)\n",
    "        actions_tensor = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards_tensor = torch.FloatTensor(rewards).to(device)\n",
    "        dones_tensor = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "        current_q = self.model(states_tensor).gather(1, actions_tensor).squeeze(1)\n",
    "        next_q = self.model(next_states_tensor).max(1)[0]\n",
    "        target_q = rewards_tensor + self.gamma * next_q * (~dones_tensor).float()\n",
    "\n",
    "        loss = self.criterion(current_q, target_q.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            self.episodes += 1\n",
    "            state, _ = self.env.reset()\n",
    "            state = self._reshape(state)\n",
    "            treward = 0\n",
    "            for f in range(1, 5000):\n",
    "                self.f = f\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, _ = self.env.step(action)\n",
    "                treward += reward\n",
    "                next_state = self._reshape(next_state)\n",
    "                self.memory.append((state, action, next_state, reward, done))\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    self.trewards.append(treward)\n",
    "                    self.max_treward = max(self.max_treward, treward)\n",
    "                    templ = f'episode={self.episodes:4d} | '\n",
    "                    templ += f'treward={treward:7.3f} | max={self.max_treward:7.3f}'\n",
    "                    print(templ, end='\\r')\n",
    "                    break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "            print()\n",
    "\n",
    "    def test(self, episodes, min_accuracy=0.0, min_performance=0.0, verbose=True, full=True):\n",
    "        # Backup and set environment thresholds\n",
    "        ma = getattr(self.env, 'min_accuracy', None)\n",
    "        if hasattr(self.env, 'min_accuracy'):\n",
    "            self.env.min_accuracy = min_accuracy\n",
    "        mp = None\n",
    "        if hasattr(self.env, 'min_performance'):\n",
    "            mp = self.env.min_performance\n",
    "            self.env.min_performance = min_performance\n",
    "            self.performances = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = self._reshape(state)\n",
    "            for f in range(1, 5001):\n",
    "                action = self.act(state)\n",
    "                state, reward, done, trunc, _ = self.env.step(action)\n",
    "                state = self._reshape(state)\n",
    "                if done:\n",
    "                    templ = f'total reward={f:4d} | accuracy={self.env.accuracy:.3f}'\n",
    "                    if hasattr(self.env, 'min_performance'):\n",
    "                        self.performances.append(self.env.performance)\n",
    "                        templ += f' | performance={self.env.performance:.3f}'\n",
    "                    if verbose:\n",
    "                        if full:\n",
    "                            print(templ)\n",
    "                        else:\n",
    "                            print(templ, end='\\r')\n",
    "                    break\n",
    "        # Restore environment thresholds\n",
    "        if hasattr(self.env, 'min_accuracy') and ma is not None:\n",
    "            self.env.min_accuracy = ma\n",
    "        if mp is not None:\n",
    "            self.env.min_performance = mp\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "# 1 = X, 2 = Y, 3 = Z\n",
    "investing = Investing('.SPX', '.VIX', 'XAU=', steps=days)\n",
    "#investing.data.plot(lw=1, style=['g--', 'b:', 'm-.'])\n",
    "#plt.ylabel('price');"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "f82e0d56-23ad-4f2d-82f9-1604c8c16e15",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:17:50.467660Z",
     "start_time": "2025-09-30T19:17:50.461059Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8140e63-2085-4058-933f-d1231bd86b4f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:17:50.546491Z",
     "start_time": "2025-09-30T19:17:50.528409Z"
    }
   },
   "source": [
    "random.seed(100)\n",
    "np.random.seed(100)\n",
    "torch.manual_seed(100)\n",
    "torch.cuda.manual_seed_all(100)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4ce12eee-f877-47fb-8c8d-844fa19d82a9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:18:02.966323Z",
     "start_time": "2025-09-30T19:17:50.615998Z"
    }
   },
   "source": "agent = InvestingAgent('3AC', feature=None, n_features=6, env=investing, hu=128, lr=0.00025)",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "d4dd8c82-c56e-4d60-ad64-8923b9131eb1",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:18:02.998137Z",
     "start_time": "2025-09-30T19:18:02.990377Z"
    }
   },
   "source": [
    "episodes = 10"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "52097e3c-c467-493e-a566-e7ade97e60c4",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:18:56.043197Z",
     "start_time": "2025-09-30T19:18:03.021737Z"
    }
   },
   "source": [
    "%time agent.learn(episodes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=   1 | treward=  2.294 | max=  2.294\r\n",
      "episode=   2 | treward=  2.723 | max=  2.723\r\n",
      "episode=   3 | treward=  1.509 | max=  2.723\r\n",
      "episode=   4 | treward=  5.431 | max=  5.431\r\n",
      "episode=   5 | treward= -3.476 | max=  5.431\r\n",
      "episode=   6 | treward=  4.838 | max=  5.431\r\n",
      "episode=   7 | treward= -0.033 | max=  5.431\r\n",
      "episode=   8 | treward=  3.942 | max=  5.431\r\n",
      "episode=   9 | treward=  2.227 | max=  5.431\r\n",
      "episode=  10 | treward=  3.377 | max=  5.431\r\n",
      "CPU times: total: 12.6 s\n",
      "Wall time: 53 s\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e80a1502-4f87-421f-95d3-bf31024c845c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:18:56.091567Z",
     "start_time": "2025-09-30T19:18:56.062035Z"
    }
   },
   "source": [
    "agent.epsilon"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9752793831785673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "098d2ed0-39ab-4fae-98b2-fa0df75ca4ae",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:18:56.125941Z",
     "start_time": "2025-09-30T19:18:56.112181Z"
    }
   },
   "source": [
    "agent.env.portfolios = pd.DataFrame()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "7b3403cb-7d9e-4464-a8d0-6b8110b49b09",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:20.154104Z",
     "start_time": "2025-09-30T19:18:56.144465Z"
    }
   },
   "source": [
    "%time agent.test(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=10 | total reward=-1.42\r\n",
      "CPU times: total: 43.5 s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "bd92a22f-9e24-4ee1-911d-e0ba8d2178cd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:20.935200Z",
     "start_time": "2025-09-30T19:20:20.921452Z"
    }
   },
   "source": [
    "agent.env.portfolios.groupby('e')[\n",
    "    ['xt', 'yt', 'zt']].mean().mean()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xt    0.947857\n",
       "yt    0.016547\n",
       "zt    0.035595\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "bb9faeb4-1a3c-4330-81be-67dd5be7cadd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.090673Z",
     "start_time": "2025-09-30T19:20:21.076826Z"
    }
   },
   "source": [
    "agent.env.portfolios.groupby('e')[\n",
    "    ['Xt', 'Yt', 'Zt', 'pv']].last().mean()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xt    1.151945\n",
       "Yt    1.070904\n",
       "Zt    1.118942\n",
       "pv    1.232462\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "0a0cbe6b-7b51-4545-8a53-756ffe66345b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.114499Z",
     "start_time": "2025-09-30T19:20:21.105608Z"
    }
   },
   "source": [
    "def get_r(n):\n",
    "    r = agent.env.portfolios[\n",
    "        agent.env.portfolios['e'] == n\n",
    "        ].set_index('date')\n",
    "    return r"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "19fe4135-09d8-4652-a1b3-09e1624bab5b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.293710Z",
     "start_time": "2025-09-30T19:20:21.275642Z"
    }
   },
   "source": [
    "n = min(agent.env.portfolios['e']) + 1\n",
    "n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "1aa6c38d-5fc1-438f-866d-5d51611c57c9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.402732Z",
     "start_time": "2025-09-30T19:20:21.375529Z"
    }
   },
   "source": [
    "r = get_r(n)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "57c3c1f6-737d-4824-b314-29384800455b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.482718Z",
     "start_time": "2025-09-30T19:20:21.467149Z"
    }
   },
   "source": [
    "r[['xt', 'yt', 'zt']].mean()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xt    0.947162\n",
       "yt    0.008528\n",
       "zt    0.044310\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "0407142c-d54e-4d1c-bcbe-f836c507bc18",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.582046Z",
     "start_time": "2025-09-30T19:20:21.568374Z"
    }
   },
   "source": [
    "r[['xt', 'yt', 'zt']].std()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xt    0.132251\n",
       "yt    0.044254\n",
       "zt    0.106386\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "06e69a4d-8412-4ee9-9f21-bc0f7fc471e9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.703548Z",
     "start_time": "2025-09-30T19:20:21.688972Z"
    }
   },
   "source": "#r[['xt', 'yt', 'zt']].plot(title='ALLOCATIONS [%]', style=['g--', 'b:', 'm-.'], lw=1, grid=True).plt.ylabel('allocation')",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "cca985ce-e258-4803-89ea-191c73d88502",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.793685Z",
     "start_time": "2025-09-30T19:20:21.774659Z"
    }
   },
   "source": [
    "cols = ['Xt', 'Yt', 'Zt', 'pv']"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "2edb5393-b888-4649-a7e5-5cd4271a5977",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.872250Z",
     "start_time": "2025-09-30T19:20:21.857573Z"
    }
   },
   "source": [
    "sub = r[cols]"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "06b19cda-4931-4131-8d4c-5f7ea828c73f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:21.966191Z",
     "start_time": "2025-09-30T19:20:21.954619Z"
    }
   },
   "source": [
    "rets = sub.iloc[-1] / sub.iloc[0] - 1\n",
    "rets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xt    0.363015\n",
       "Yt    0.837442\n",
       "Zt    0.177179\n",
       "pv    0.531629\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "63b71c43-208d-43b7-b914-3ce7748f70db",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:22.056195Z",
     "start_time": "2025-09-30T19:20:22.032616Z"
    }
   },
   "source": [
    "stds = sub.pct_change().std() * math.sqrt(252)\n",
    "stds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xt    0.262097\n",
       "Yt    1.540253\n",
       "Zt    0.166215\n",
       "pv    0.231560\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "5c818674-9b9e-4ddd-99a8-fe2e3bf34dbd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:27.819241Z",
     "start_time": "2025-09-30T19:20:27.800212Z"
    }
   },
   "source": [
    "rets / stds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xt    1.385041\n",
       "Yt    0.543704\n",
       "Zt    1.065963\n",
       "pv    2.295856\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "512cea65-a5b3-4563-a8b1-a12ff951369b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.072590Z",
     "start_time": "2025-09-30T19:20:28.058820Z"
    }
   },
   "source": [
    "#sub.plot(style=['g--', 'b:', 'm-.', 'r-'], lw=1)\n",
    "#plt.ylabel('value');"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "bff144c2-09b5-4563-9209-506d8f457956",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.232201Z",
     "start_time": "2025-09-30T19:20:28.216364Z"
    }
   },
   "source": [
    "sharpe = pd.DataFrame()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "c10f8db0-e3b9-4465-bd87-5665da9a5b6e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.277525Z",
     "start_time": "2025-09-30T19:20:28.263759Z"
    }
   },
   "source": [
    "def calculate_sr():\n",
    "    for n in set(investing.portfolios['e']):\n",
    "        r = get_r(n)\n",
    "        sub = r[cols]\n",
    "        rets = sub.iloc[-1] / sub.iloc[0] - 1\n",
    "        stds = sub.pct_change().std() * math.sqrt(252)\n",
    "        sharpe[n] = rets / stds"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "cde4fd3c-69d9-420a-91a0-3600c3d28f29",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.384352Z",
     "start_time": "2025-09-30T19:20:28.324886Z"
    }
   },
   "source": [
    "calculate_sr()"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "aaea3385-4a04-4898-81f7-0da8745a7ef5",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.465156Z",
     "start_time": "2025-09-30T19:20:28.451392Z"
    }
   },
   "source": [
    "sharpe.round(2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      11    12    13    14    15    16    17    18    19    20\n",
       "Xt  1.19  1.39  2.26  0.33  2.75 -0.11  0.62  1.06 -0.09  0.37\n",
       "Yt  0.14  0.54 -0.29  0.21  0.03 -0.43  0.04  0.10 -0.20  0.12\n",
       "Zt  1.64  1.07  0.79 -0.56 -0.70  1.02  0.13  3.61  0.58  0.61\n",
       "pv  0.99  2.30  2.63  2.49  5.36 -0.25  0.71  2.00 -0.31  0.15"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Xt</th>\n",
       "      <td>1.19</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.06</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yt</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zt</th>\n",
       "      <td>1.64</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pv</th>\n",
       "      <td>0.99</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.49</td>\n",
       "      <td>5.36</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "cce96cc7-7bef-424b-b69c-154e16fed812",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.548784Z",
     "start_time": "2025-09-30T19:20:28.535018Z"
    }
   },
   "source": [
    "sharpe.mean(axis=1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xt    0.976497\n",
       "Yt    0.026487\n",
       "Zt    0.817586\n",
       "pv    1.606219\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "2a007c1e-c76d-45c0-a947-efa5220e2f6d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.631990Z",
     "start_time": "2025-09-30T19:20:28.618961Z"
    }
   },
   "source": [
    "((sharpe.loc['pv'] > sharpe.loc['Xt']) &\n",
    " (sharpe.loc['pv'] > sharpe.loc['Yt']) &\n",
    " (sharpe.loc['pv'] > sharpe.loc['Zt'])).value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5\n",
       "True     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "e62b75bf-382e-47be-ab78-7a309572b8b1",
   "metadata": {},
   "source": [
    "## Portefeuille égal pondéré (référence)\n\nLa stratégie buy-and-hold égal-pondérée sert de baseline : elle fournit un repère de Sharpe/volatilité et permet de quantifier le gain du DQN en termes de rendement ajusté du risque et de stabilité (drawdown).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a980883-7ce4-4543-bbf4-8edf83570e4a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.731414Z",
     "start_time": "2025-09-30T19:20:28.713877Z"
    }
   },
   "source": [
    "agent.opt_action = lambda state: np.ones(3) / 3"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "c5c5afeb-283b-40cc-8b04-8b700df447af",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:20:28.870880Z",
     "start_time": "2025-09-30T19:20:28.857881Z"
    }
   },
   "source": [
    "agent.env.portfolios = pd.DataFrame()"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "2222a0d5-38a1-4cb4-94d9-9a969086084b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:21:19.998399Z",
     "start_time": "2025-09-30T19:20:28.939225Z"
    }
   },
   "source": [
    "%time agent.test(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=10 | total reward=1.45\r\n",
      "CPU times: total: 7.92 s\n",
      "Wall time: 51.1 s\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "e80f3820-a156-46c6-b754-c7ce504b83a7",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:21:20.029745Z",
     "start_time": "2025-09-30T19:21:20.018673Z"
    }
   },
   "source": [
    "sharpe = pd.DataFrame()"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "9b66a209-4439-49df-adb3-c8e5e17d94e8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:21:20.104213Z",
     "start_time": "2025-09-30T19:21:20.051580Z"
    }
   },
   "source": [
    "calculate_sr()"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "434802f9-f318-4dac-8ca7-131f69275264",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:21:20.168938Z",
     "start_time": "2025-09-30T19:21:20.138296Z"
    }
   },
   "source": [
    "sharpe.round(2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      21    22    23    24    25    26    27    28    29    30\n",
       "Xt  1.91  0.42  2.56 -0.05  1.68  2.21  1.65  2.03  3.43  0.67\n",
       "Yt  0.02  0.04  0.34 -0.53  0.02 -0.34  0.26  0.25 -0.03  0.04\n",
       "Zt  1.57 -0.25  0.69  0.88  0.35  1.27  2.33  1.54 -0.98 -0.38\n",
       "pv  1.96  1.35  1.91  0.39  1.41  0.91  2.98  2.88  1.05  1.38"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Xt</th>\n",
       "      <td>1.91</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.56</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yt</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zt</th>\n",
       "      <td>1.57</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.54</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pv</th>\n",
       "      <td>1.96</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.98</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "c42f87f1-fe0b-4bd2-bd26-a90544bd5efb",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:21:20.294224Z",
     "start_time": "2025-09-30T19:21:20.281103Z"
    }
   },
   "source": [
    "sharpe.mean(axis=1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xt    1.651726\n",
       "Yt    0.006942\n",
       "Zt    0.702165\n",
       "pv    1.620502\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "84379d43-e3a6-49e6-8d2f-8be3d6102dcd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-30T19:21:20.385110Z",
     "start_time": "2025-09-30T19:21:20.375109Z"
    }
   },
   "source": [
    "((sharpe.loc['pv'] > sharpe.loc['Xt']) &\n",
    " (sharpe.loc['pv'] > sharpe.loc['Yt']) &\n",
    " (sharpe.loc['pv'] > sharpe.loc['Zt'])).value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     5\n",
       "False    5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
   "metadata": {},
   "source": [
    "## Conclusion et extensions\n\nCe prototype d'allocation RL montre comment un DQN peut ajuster dynamiquement les poids sous contrainte de coûts. Pour aller vers un cadre plus réaliste : intégrer des covariances empiriques, des frictions variables et tester des modèles génératifs plus riches (vol stochastique, régimes de marché). Les mêmes principes serviront pour un module deep hedging multi-actifs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}